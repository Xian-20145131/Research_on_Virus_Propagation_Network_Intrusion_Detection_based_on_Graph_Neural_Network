{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from metrics import multi_evaluate\n",
    "import time\n",
    "import copy\n",
    "from torch_geometric.nn import GCNConv,GATConv,SAGEConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx]\n",
    "        target = self.target[idx]\n",
    "        # sample = {'features': features, 'target': target}\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(raw_data, cols):\n",
    "    df = copy.deepcopy(raw_data)\n",
    "    label = 'label'\n",
    "    target = df.pop(label)\n",
    "    df = df[cols]  \n",
    "    \n",
    "    numerical_features = [x for x in df.columns if df[x].dtype == np.float64 or df[x].dtype == np.int64]\n",
    "    categorical_features = [x for x in df.columns if df[x].dtype == object]\n",
    "    \n",
    "    # convert object to int\n",
    "    lbe = LabelEncoder()\n",
    "    for feat in categorical_features:\n",
    "        df[feat] = lbe.fit_transform(df[feat])\n",
    "    \n",
    "    # normalize the features\n",
    "    mms = MinMaxScaler()\n",
    "    df[numerical_features] = mms.fit_transform(df[numerical_features])\n",
    "    \n",
    "    return df, target, numerical_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(features, target, batch_size=64):\n",
    "    dataset = CustomDataset(features.values, target.values, transform=ToTensor(), target_transform=ToTensor())\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "        \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    \n",
    "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, device, idx2label):\n",
    "    with torch.no_grad():\n",
    "        input = torch.from_numpy(dataloader.dataset.data).float().to(device)\n",
    "        out = model(input)\n",
    "    y_pred = out.argmax(1).to('cpu').numpy()\n",
    "    y_test = dataloader.dataset.target\n",
    "    return multi_evaluate(y_test, y_pred, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(inp, out, dropout):\n",
    "    \"\"\"\n",
    "    linear model module by nn.sequential\n",
    "    :param inp: int, linear model input dimensio\n",
    "    :param out: int, linear model output dimension\n",
    "    :param dropout: float dropout probability for linear layer\n",
    "    :return: tensor\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(inp, out),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(dropout)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 channels_gnn,\n",
    "                 channels_mlp=None,\n",
    "                 num_class=None,\n",
    "                 heads=1,\n",
    "                 mp_nn=\"gcn\",\n",
    "                 bias_gnn=True,\n",
    "                 bias_mlp=True):\n",
    "        super().__init__()\n",
    "\n",
    "        channels_gnn.append(num_class)\n",
    "        \n",
    "        self.channels_gnn = channels_gnn\n",
    "        self.mp_nn = mp_nn\n",
    "        self.bias_gnn = bias_gnn\n",
    "        self.heads = heads\n",
    "        self.gnn = nn.ModuleList()\n",
    "        \n",
    "        for channel_no in range(len(channels_gnn)-1):\n",
    "            if self.mp_nn == \"gcn\":\n",
    "                self.gnn.append(GCNConv(in_channels=channels_gnn[channel_no], out_channels=channels_gnn[channel_no+1], bias=bias_gnn))\n",
    "                \n",
    "            elif self.mp_nn == \"gat\":\n",
    "                self.gnn.append(GATConv(in_channels = channels_gnn[channel_no], out_channels=channels_gnn[channel_no+1], heads = self.heads,bias=bias_gnn,concat=False))\n",
    "\n",
    "            elif self.mp_nn == \"sg\":\n",
    "                self.gnn.append(SAGEConv(in_channels=channels_gnn[channel_no], out_channels=channels_gnn[channel_no+1], bias = bias_gnn))\n",
    "            \n",
    "            else:\n",
    "                raise Exception(\"Check GNN type!\")\n",
    "\n",
    "        \n",
    "        # self.channels_mlp = channels_mlp        \n",
    "        # self.bias_mlp = bias_mlp\n",
    "        # self.num_class = num_class\n",
    "        # self.linear = nn.ModuleList()\n",
    "        # self.channels_mlp.insert(0,channels_gnn[-1])\n",
    "        # self.channels_mlp.append(num_class)\n",
    "\n",
    "        # for channel_no in range(0,len(channels_mlp)-1):\n",
    "        #     self.linear.append(nn.Linear(channels_mlp[channel_no],channels_mlp[channel_no+1], bias=bias_mlp))\n",
    "        \n",
    "\n",
    "    def model_parameters(self, model):\n",
    "        return model.state_dict()\n",
    "\n",
    "    def weight_update_gnn(self, wgt_add):\n",
    "        assert len(self.gnn) -1== len(wgt_add), \"Match number of GNNs and feature additions\"\n",
    "\n",
    "\n",
    "        # for i in range(1,len(self.channels_gnn)):\n",
    "        #     self.channels_gnn[i] = self.channels_gnn[i] + wgt_add[i-1]\n",
    "\n",
    "        for i in range(1,len(self.channels_gnn)-1):\n",
    "            self.channels_gnn[i] = self.channels_gnn[i] + wgt_add[i-1]\n",
    "\n",
    "        for i in range(len(self.gnn)):\n",
    "            model_param = self.model_parameters(self.gnn[i])\n",
    "            \n",
    "            if self.mp_nn == \"gcn\":\n",
    "                self.gnn[i] = GCNConv(in_channels = self.channels_gnn[i], out_channels = self.channels_gnn[i+1], bias=self.bias_gnn)\n",
    "\n",
    "            elif self.mp_nn == \"gat\":\n",
    "                self.gnn[i] = GATConv(in_channels = self.channels_gnn[i], out_channels = self.channels_gnn[i+1], heads = self.heads, bias=self.bias_gnn, concat=False)\n",
    "\n",
    "            elif self.mp_nn == \"sg\":\n",
    "                self.gnn[i] = SAGEConv(in_channels=self.channels_gnn[i], out_channels=self.channels_gnn[i+1], bias = self.bias_gnn)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.mp_nn == \"gcn\":\n",
    "                    self.gnn[i].lin.weight[0:model_param[\"lin.weight\"].shape[0] , 0:model_param[\"lin.weight\"].shape[1]] = model_param[\"lin.weight\"]\n",
    "                    if self.bias_gnn:\n",
    "                        self.gnn[i].bias[0:model_param[\"bias\"].shape[0]] = model_param[\"bias\"]\n",
    "                        \n",
    "                elif self.mp_nn == \"gat\":\n",
    "                    self.gnn[i].att_src[0,0:self.heads,0:model_param['att_src'].shape[2]] = model_param['att_src'][0,0:self.heads]\n",
    "                    self.gnn[i].att_dst[0,0:self.heads,0:model_param['att_dst'].shape[2]] = model_param['att_dst'][0,0:self.heads]\n",
    "                    self.gnn[i].lin_src.weight[0:model_param[\"lin_src.weight\"].shape[0] , 0:model_param[\"lin_src.weight\"].shape[1]] = model_param[\"lin_src.weight\"]\n",
    "                    if self.bias_gnn:\n",
    "                        self.gnn[i].bias[0:model_param[\"bias\"].shape[0]] = model_param[\"bias\"]\n",
    "\n",
    "                elif self.mp_nn == \"sg\":\n",
    "                    self.gnn[i].lin_l.weight[0:model_param[\"lin_l.weight\"].shape[0] , 0:model_param[\"lin_l.weight\"].shape[1]] = model_param[\"lin_l.weight\"]\n",
    "                    self.gnn[i].lin_r.weight[0:model_param[\"lin_r.weight\"].shape[0] , 0:model_param[\"lin_r.weight\"].shape[1]] = model_param[\"lin_r.weight\"]\n",
    "                    if self.bias_gnn:\n",
    "                        self.gnn[i].lin_l.bias[0:model_param[\"lin_l.bias\"].shape[0]] = model_param[\"lin_l.bias\"]\n",
    "                       \n",
    "    # def weight_update_mlp(self, wgt_add):\n",
    "    #     assert len(self.linear)-1 == len(wgt_add), \"Match number of Linear layers and node additions\"\n",
    "    #     assert self.channels_mlp[-1] == self.num_class, \"MLP output not match class number\"\n",
    "\n",
    "    #     self.channels_mlp[0] = self.channels_gnn[-1] #Change here!\n",
    "        \n",
    "    #     for i in range(1,len(self.channels_mlp)-1):\n",
    "    #         self.channels_mlp[i] = self.channels_mlp[i] + wgt_add[i-1]\n",
    "\n",
    "    #     for i in range(len(self.linear)):\n",
    "    #         model_param = self.model_parameters(self.linear[i])\n",
    "    #         self.linear[i] = nn.Linear(self.channels_mlp[i], self.channels_mlp[i+1], bias=self.bias_mlp)\n",
    "    #         with torch.no_grad():\n",
    "    #             self.linear[i].weight[0:model_param[\"weight\"].shape[0] , 0:model_param[\"weight\"].shape[1]] = model_param[\"weight\"]\n",
    "    #             if self.bias_mlp:\n",
    "    #                 self.linear[i].bias[0:model_param[\"bias\"].shape[0]] = model_param[\"bias\"]\n",
    "\n",
    "    def weight_update(self, wgt_gnn=None, wgt_mlp=None):\n",
    "        self.weight_update_gnn(wgt_gnn)\n",
    "        # self.weight_update_mlp(wgt_mlp)             \n",
    "  \n",
    "    def forward(self,x,edge_index):\n",
    "        for i in range(len(self.gnn)):\n",
    "            x = self.gnn[i](x, edge_index)\n",
    "            x = F.leaky_relu(x)\n",
    "            x =  F.dropout(x, training=self.training)\n",
    "\n",
    "        # for i in range(len(self.linear)):\n",
    "        #     x = self.linear[i](x)\n",
    "        #     x =  F.dropout(x, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "# def equal_(self):\n",
    "#     for (n,p),(_,pc) in zip(self.model.named_parameters(),self.mc.named_parameters()):\n",
    "#         # print(i,n,p.grad,sep='\\t')\n",
    "#         if not torch.all(p.eq(pc)).data:\n",
    "#             print(n,\"\\n\", p.eq(pc),sep='\\t')\n",
    "#         else:\n",
    "#             return True\n",
    "# print(\"Initial parameters\")\n",
    "\n",
    "# edge_index = torch.tensor([[0, 1],\n",
    "#                            [1, 0],\n",
    "#                            [1, 2],\n",
    "#                            [2, 1],\n",
    "#                            ], dtype=torch.long).t().contiguous()\n",
    "\n",
    "# x = torch.tensor([[-1,2], [0,3], [1,5]], dtype=torch.float)\n",
    "\n",
    "# geo = GraphLayer(channels_gnn = [x.shape[1],3,5], channels_mlp=[8,3], num_class=5)\n",
    "\n",
    "# for n,p in geo.named_parameters():\n",
    "#     print(n,p,end=\"\\n\")\n",
    "\n",
    "# out_ini = geo(x,edge_index)\n",
    "# print(\"Output\",out_ini)\n",
    "\n",
    "# print(\"--\"*120,\"\\nUpdated parameters\")\n",
    "# wgt_add_gnn = [1,2]\n",
    "# wgt_add_mlp = [7,3]\n",
    "# geo.weight_update(wgt_add_gnn,wgt_add_mlp)\n",
    "\n",
    "# for n,p in geo.named_parameters():\n",
    "#     print(n,p,end=\"\\n\")\n",
    "\n",
    "# out_upd = geo(x,edge_index)\n",
    "# print(out_upd)\n",
    "\n",
    "# print(\"*\"*120)\n",
    "# print(\"Update again!\")\n",
    "# wgt_add_gnn = [4,2]\n",
    "# wgt_add_mlp = [7,5]\n",
    "# geo.weight_update(wgt_add_gnn,wgt_add_mlp)\n",
    "\n",
    "# for n,p in geo.named_parameters():\n",
    "#     print(n,p,end=\"\\n\")\n",
    "# print(\"-\"*100)\n",
    "# out_upd = geo(x,edge_index)\n",
    "# print(out_upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data/kdd99/kddcup_10p_preprocessing_five.csv')\n",
    "type2idx = {type: idx for idx, type in enumerate(raw_data['label'].unique())}\n",
    "idx2type = {idx: type for idx, type in enumerate(raw_data['label'].unique())}\n",
    "raw_data['label'] = raw_data['label'].apply(lambda x: type2idx[x])\n",
    "raw_data.label = raw_data.label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.read_csv('./importance/kdd99/five/importance_70.csv', index_col=0)\n",
    "cols = importance[:20].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target, numerical_features, categorical_features = preprocessing(raw_data, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = {feat : idx for idx, feat in enumerate(features.columns)}\n",
    "embedding_dim = 5\n",
    "embedding_feat = {feat: (features[feat].value_counts().count(),embedding_dim) for feat in categorical_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "cv_result = dict()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for idx, (train_idx, test_idx) in enumerate(kf.split(features, target)):\n",
    "    result = dict()\n",
    "    x_train, x_test, y_train, y_test = features.loc[train_idx], features.loc[test_idx], target.loc[train_idx], target.loc[test_idx]\n",
    "    train_dataloader = create_dataloader(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "    test_dataloader = create_dataloader(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    device = 'cuda'\n",
    "    model = GNN_embedding(feat_dict, embedding_feat, [512, 512, 512], [0, 0, 0], 5).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    epochs = 20\n",
    "    training_time = 0.0\n",
    "    testing_time = 0.0\n",
    "    for t in range(epochs):\n",
    "        print(f'----------Epoch {t+1}----------')\n",
    "        training_time += train(train_dataloader, model, loss_fn, optimizer)\n",
    "        testing_time += test(test_dataloader, model, loss_fn)\n",
    "    result = evaluate(test_dataloader, model, device, idx2type)\n",
    "    for key in type2idx.keys():\n",
    "        result[key]['training_time'] = training_time\n",
    "        result[key]['testing_time'] = testing_time\n",
    "    cv_result[idx+1] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result = dict()\n",
    "for sample_rate in range(10, 110, 10):\n",
    "    importance = pd.read_csv('./importance/kdd99/five/importance_' + str(sample_rate) + '.csv', index_col=0)\n",
    "    cer_sample_result = dict()\n",
    "    print(f'----------Sample rate is {sample_rate}----------')\n",
    "\n",
    "    for i in range(1, len(importance)+1):\n",
    "        print(f'----------Feature number is {i}----------')\n",
    "        cols = importance.index[:i]\n",
    "        features, target, numerical_features, categorical_features = preprocessing(raw_data, cols)\n",
    "\n",
    "        feat_dict = {feat : idx for idx, feat in enumerate(features.columns)}\n",
    "        embedding_dim = 5\n",
    "        embedding_feat = {feat: (features[feat].value_counts().count(),embedding_dim) for feat in categorical_features}\n",
    "        BATCH_SIZE = 1024\n",
    "        cv_result = dict()\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        for idx, (train_idx, test_idx) in enumerate(kf.split(features, target)):\n",
    "            result = dict()\n",
    "            x_train, x_test, y_train, y_test = features.loc[train_idx], features.loc[test_idx], target.loc[train_idx], target.loc[test_idx]\n",
    "            train_dataloader = create_dataloader(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "            test_dataloader = create_dataloader(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "            device = 'cuda'\n",
    "            model = GNN_embedding(feat_dict, embedding_feat, [512, 512, 512], [0, 0, 0], 5).to(device)\n",
    "            \n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "            epochs = 10\n",
    "            training_time = 0.0\n",
    "            testing_time = 0.0\n",
    "            for t in range(epochs):\n",
    "                # print(f'----------Epoch {t+1}----------')\n",
    "                training_time += train(train_dataloader, model, loss_fn, optimizer)\n",
    "                testing_time += test(test_dataloader, model, loss_fn)\n",
    "            result = evaluate(test_dataloader, model, device, idx2type)\n",
    "            for key in type2idx.keys():\n",
    "                result[key]['training_time'] = training_time\n",
    "                result[key]['testing_time'] = testing_time\n",
    "            cv_result[idx+1] = result\n",
    "        cer_sample_result[i] = cv_result\n",
    "    total_result[sample_rate] = cer_sample_result\n",
    "    tmp = pd.DataFrame.from_dict({(i, j, k): cer_sample_result[i][j][k] for i in cer_sample_result.keys() for j in cer_sample_result[i].keys() for k in cer_sample_result[i][j].keys()}, orient='index').to_csv('./result_sample_rate_' + str(sample_rate) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict({(i, j, k, v): total_result[i][j][k][v] for i  in total_result.keys() for j in total_result[i].keys() for k in total_result[i][j].keys() for v in total_result[i][j][k].keys()}, orient='index').to_csv('./total_result_kdd99.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c5d7cd20ebd7687f00d91fcadbe129cb9df9d5118e3a22d3b436ce230f91b1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
