{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from metrics import binary_evaluate\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = './data/kdd99/kddcup_10p_preprocessing.csv'\n",
    "# test_file = './data/nslkdd/KDDTest_binary.csv'\n",
    "df = pd.read_csv(read_file)\n",
    "# test_df = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [x for x in df.columns if df[x].dtype == np.float64 or df[x].dtype == np.int64]\n",
    "categorical_features = [x for x in df.columns if df[x].dtype == object]\n",
    "def discretization(x):\n",
    "    mapp = dict(zip(x.unique().tolist(),\n",
    "         range(len(x.unique().tolist()))))\n",
    "    def mapfunction(y):\n",
    "        if y in mapp:\n",
    "            return mapp[y]\n",
    "        else:\n",
    "            return -1\n",
    "    return mapfunction\n",
    "for i in categorical_features:\n",
    "    df[i] = df[i].apply(discretization(df[i]))\n",
    "    # test_df[i] = test_df[i].apply(discretization(test_df[i]))\n",
    "categorical_features.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df\n",
    "target = features.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:24:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=40, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.Series(clf.feature_importances_, index=features.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = importance[:21].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from metrics import binary_evaluate\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx]\n",
    "        target = self.target[idx]\n",
    "        # sample = {'features': features, 'target': target}\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(raw_data, cols):\n",
    "    df = copy.deepcopy(raw_data)\n",
    "    label = 'label'\n",
    "    target = df.pop(label)\n",
    "    df = df[cols]  \n",
    "    \n",
    "    numerical_features = [x for x in df.columns if df[x].dtype == np.float64 or df[x].dtype == np.int64]\n",
    "    categorical_features = [x for x in df.columns if df[x].dtype == object]\n",
    "    \n",
    "    # convert object to int\n",
    "    lbe = LabelEncoder()\n",
    "    for feat in categorical_features:\n",
    "        df[feat] = lbe.fit_transform(df[feat])\n",
    "    \n",
    "    # normalize the features\n",
    "    mms = MinMaxScaler()\n",
    "    df[numerical_features] = mms.fit_transform(df[numerical_features])\n",
    "    \n",
    "    return df, target, numerical_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(features, target, batch_size=64):\n",
    "    dataset = CustomDataset(features.values, target.values, transform=ToTensor(), target_transform=ToTensor())\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "        \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    \n",
    "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, device):\n",
    "    with torch.no_grad():\n",
    "        input = torch.from_numpy(dataloader.dataset.data).float().to(device)\n",
    "        out = model(input)\n",
    "    y_pred = out.argmax(1).to('cpu').numpy()\n",
    "    y_test = dataloader.dataset.target\n",
    "    return binary_evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "features[categorical_features] = mms.fit_transform(features[categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145585, 21)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 2----------\n",
      "----------Epoch 3----------\n",
      "----------Epoch 4----------\n",
      "----------Epoch 5----------\n",
      "----------Epoch 6----------\n",
      "----------Epoch 7----------\n",
      "----------Epoch 8----------\n",
      "----------Epoch 9----------\n",
      "----------Epoch 10----------\n",
      "----------Epoch 11----------\n",
      "----------Epoch 12----------\n",
      "----------Epoch 13----------\n",
      "----------Epoch 14----------\n",
      "----------Epoch 15----------\n",
      "----------Epoch 16----------\n",
      "----------Epoch 17----------\n",
      "----------Epoch 18----------\n",
      "----------Epoch 19----------\n",
      "----------Epoch 20----------\n",
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 2----------\n",
      "----------Epoch 3----------\n",
      "----------Epoch 4----------\n",
      "----------Epoch 5----------\n",
      "----------Epoch 6----------\n",
      "----------Epoch 7----------\n",
      "----------Epoch 8----------\n",
      "----------Epoch 9----------\n",
      "----------Epoch 10----------\n",
      "----------Epoch 11----------\n",
      "----------Epoch 12----------\n",
      "----------Epoch 13----------\n",
      "----------Epoch 14----------\n",
      "----------Epoch 15----------\n",
      "----------Epoch 16----------\n",
      "----------Epoch 17----------\n",
      "----------Epoch 18----------\n",
      "----------Epoch 19----------\n",
      "----------Epoch 20----------\n",
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 2----------\n",
      "----------Epoch 3----------\n",
      "----------Epoch 4----------\n",
      "----------Epoch 5----------\n",
      "----------Epoch 6----------\n",
      "----------Epoch 7----------\n",
      "----------Epoch 8----------\n",
      "----------Epoch 9----------\n",
      "----------Epoch 10----------\n",
      "----------Epoch 11----------\n",
      "----------Epoch 12----------\n",
      "----------Epoch 13----------\n",
      "----------Epoch 14----------\n",
      "----------Epoch 15----------\n",
      "----------Epoch 16----------\n",
      "----------Epoch 17----------\n",
      "----------Epoch 18----------\n",
      "----------Epoch 19----------\n",
      "----------Epoch 20----------\n",
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 2----------\n",
      "----------Epoch 3----------\n",
      "----------Epoch 4----------\n",
      "----------Epoch 5----------\n",
      "----------Epoch 6----------\n",
      "----------Epoch 7----------\n",
      "----------Epoch 8----------\n",
      "----------Epoch 9----------\n",
      "----------Epoch 10----------\n",
      "----------Epoch 11----------\n",
      "----------Epoch 12----------\n",
      "----------Epoch 13----------\n",
      "----------Epoch 14----------\n",
      "----------Epoch 15----------\n",
      "----------Epoch 16----------\n",
      "----------Epoch 17----------\n",
      "----------Epoch 18----------\n",
      "----------Epoch 19----------\n",
      "----------Epoch 20----------\n",
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 2----------\n",
      "----------Epoch 3----------\n",
      "----------Epoch 4----------\n",
      "----------Epoch 5----------\n",
      "----------Epoch 6----------\n",
      "----------Epoch 7----------\n",
      "----------Epoch 8----------\n",
      "----------Epoch 9----------\n",
      "----------Epoch 10----------\n",
      "----------Epoch 11----------\n",
      "----------Epoch 12----------\n",
      "----------Epoch 13----------\n",
      "----------Epoch 14----------\n",
      "----------Epoch 15----------\n",
      "----------Epoch 16----------\n",
      "----------Epoch 17----------\n",
      "----------Epoch 18----------\n",
      "----------Epoch 19----------\n",
      "----------Epoch 20----------\n"
     ]
    }
   ],
   "source": [
    "cv_result = dict()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for idx, (train_idx, test_idx) in enumerate(kf.split(features, target)):\n",
    "    result = dict()\n",
    "    x_train, x_test, y_train, y_test = features.loc[train_idx], features.loc[test_idx], target.loc[train_idx], target.loc[test_idx]\n",
    "    train_dataloader = create_dataloader(x_train, y_train, batch_size=1024)\n",
    "    test_dataloader = create_dataloader(x_test, y_test, batch_size=1024)\n",
    "    device = 'cuda'\n",
    "    model = NeuralNetwork(features.shape[1]).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    epochs = 20\n",
    "    training_time = 0.0\n",
    "    testing_time = 0.0\n",
    "    for t in range(epochs):\n",
    "        # print(f'----------Epoch {t+1}----------')\n",
    "        training_time += train(train_dataloader, model, loss_fn, optimizer)\n",
    "        testing_time += test(test_dataloader, model, loss_fn)\n",
    "    result = evaluate(test_dataloader, model, device)\n",
    "    result['training_time'] = training_time\n",
    "    result['testing_time'] = testing_time\n",
    "    cv_result[idx+1] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_result).mean(1).to_csv('./result/kdd99/binary_embedding/xgboost_dnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c5d7cd20ebd7687f00d91fcadbe129cb9df9d5118e3a22d3b436ce230f91b1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
