{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file = './data/kdd99/kddcup_10p_preprocessing_five.csv'\n",
    "df = pd.read_csv(read_file)\n",
    "target = df.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {idx: label for idx, label in enumerate(target.unique())}\n",
    "label2idx = {label: idx for idx, label in enumerate(target.unique())}\n",
    "target = target.apply(lambda x: label2idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [x for x in df.columns if df[x].dtype == np.float64 or df[x].dtype == np.int64]\n",
    "categorical_features = [x for x in df.columns if df[x].dtype == object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe = LabelEncoder()\n",
    "for feat in categorical_features:\n",
    "    df[feat] = lbe.fit_transform(df[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:15:45] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=40, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.Series(clf.feature_importances_, index=features.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['same_srv_rate', 'wrong_fragment', 'srv_serror_rate', 'diff_srv_rate',\n",
       "       'num_compromised', 'count', 'hot', 'serror_rate',\n",
       "       'dst_host_diff_srv_rate', 'protocol_type', 'src_bytes',\n",
       "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'logged_in',\n",
       "       'dst_host_srv_serror_rate', 'num_failed_logins', 'srv_count',\n",
       "       'dst_host_same_src_port_rate', 'root_shell', 'dst_bytes', 'service'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = importance[:21].index\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from metrics import multi_evaluate\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx]\n",
    "        target = self.target[idx]\n",
    "        # sample = {'features': features, 'target': target}\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(features, target, batch_size=64):\n",
    "    dataset = CustomDataset(features.values, target.values, transform=ToTensor(), target_transform=ToTensor())\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "        \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, device, idx2label):\n",
    "    with torch.no_grad():\n",
    "        input = torch.from_numpy(dataloader.dataset.data).float().to(device)\n",
    "        out = model(input)\n",
    "    y_pred = out.argmax(1).to('cpu').numpy()\n",
    "    y_test = dataloader.dataset.target\n",
    "    return multi_evaluate(y_test, y_pred, idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 5),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "features[selected_features] = mms.fit_transform(features[selected_features])\n",
    "features = features[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.609658  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 0.904833  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 0.904833  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 0.904833  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 0.904833  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 0.904833  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 0.904833  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 0.904833  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 0.904833  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 0.904833  [    0/116468]\n",
      "loss: 1.904831  [102400/116468]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.001306 \n",
      "\n",
      "loss: 1.609130  [    0/116469]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/Desktop/anomaly-detection/metrics.py:66: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  p_tmp = multi_matrix[i][0,0] / (multi_matrix[i][0,0] + multi_matrix[i][1,0])\n",
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.001305 \n",
      "\n",
      "loss: 1.610256  [    0/116469]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/Desktop/anomaly-detection/metrics.py:66: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  p_tmp = multi_matrix[i][0,0] / (multi_matrix[i][0,0] + multi_matrix[i][1,0])\n",
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.001308 \n",
      "\n",
      "loss: 1.610963  [    0/116469]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/Desktop/anomaly-detection/metrics.py:66: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  p_tmp = multi_matrix[i][0,0] / (multi_matrix[i][0,0] + multi_matrix[i][1,0])\n",
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.001311 \n",
      "\n",
      "loss: 1.614539  [    0/116469]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/Desktop/anomaly-detection/metrics.py:66: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  p_tmp = multi_matrix[i][0,0] / (multi_matrix[i][0,0] + multi_matrix[i][1,0])\n",
      "/home/hadoop/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n",
      "loss: 0.904833  [    0/116469]\n",
      "loss: 1.904831  [102400/116469]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.001309 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/Desktop/anomaly-detection/metrics.py:66: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  p_tmp = multi_matrix[i][0,0] / (multi_matrix[i][0,0] + multi_matrix[i][1,0])\n"
     ]
    }
   ],
   "source": [
    "cv_result = dict()\n",
    "BATCH_SIZE = 1024\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for idx, (train_idx, test_idx) in enumerate(kf.split(features, target)):\n",
    "    result = dict()\n",
    "    x_train, x_test, y_train, y_test = features.loc[train_idx], features.loc[test_idx], target.loc[train_idx], target.loc[test_idx]\n",
    "    train_dataloader = create_dataloader(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "    test_dataloader = create_dataloader(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "    device = 'cuda'\n",
    "    model = NeuralNetwork(features.shape[1]).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    epochs = 10\n",
    "    training_time = 0.0\n",
    "    testing_time = 0.0\n",
    "    for t in range(epochs):\n",
    "        # print(f'----------Epoch {t+1}----------')\n",
    "        training_time += train(train_dataloader, model, loss_fn, optimizer)\n",
    "        testing_time += test(test_dataloader, model, loss_fn)\n",
    "    result = evaluate(test_dataloader, model, device, idx2label)\n",
    "    result['training_time'] = training_time\n",
    "    result['testing_time'] = testing_time\n",
    "    cv_result[idx+1] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame.from_dict({(j, i): cv_result[i][j] for i in cv_result.keys() for j in cv_result[i].keys()}).T\n",
    "result.index.names = ['type', 'cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.groupby(['type']).mean().to_csv('./result/kdd99/five/xgboost_dnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c5d7cd20ebd7687f00d91fcadbe129cb9df9d5118e3a22d3b436ce230f91b1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
